import argparse
import json
import os

from evaluator import run_evaluation


def load_text_file(file_path: str) -> list[str]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    
    with open(file_path, 'r') as file:
        return [
            line.strip()
            for line in file.readlines()
        ]


if __name__ == "__main__":
    # Parse the arguments
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument('--input-texts', '-i', type=str, required=True, help="The file containing the input texts to the model")
    arg_parser.add_argument('--summaries', '-s', type=str, required=True, help="The file containing the summaries generated by the model")
    arg_parser.add_argument('--references', '-r', type=str, required=True, help="The file containing the references for the summaries")
    args = arg_parser.parse_args()

    # Load the input texts, summaries, and references
    input_texts = load_text_file(args.input_texts)
    summaries = load_text_file(args.summaries)
    references = load_text_file(args.references)

    # Evaluate the metrics
    results = run_evaluation(input_texts, summaries, references, device='cpu')  # TODO: use a GPU
    print(json.dumps(results, indent=2))
