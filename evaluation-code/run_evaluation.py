import argparse
import json
import os

from evaluator import run_evaluation, AVAILABLE_METRICS

# The maximum number of samples to evaluate
# This is to prevent the evaluation from taking too long
# If you want to evaluate all samples, set this to None
# It was required in our project due to the large number of samples and the time it took to evaluate them
MAX_SAMPLES = 2000


def load_text_file(file_path: str) -> list[str]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    
    with open(file_path, 'r') as file:
        file_lines = [
            line.strip()
            for line in file.readlines()
        ]

    if MAX_SAMPLES is not None:
        file_lines = file_lines[:MAX_SAMPLES]
    
    return file_lines


if __name__ == "__main__":
    # Parse the arguments
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument('--input-texts', '-i', type=str, required=True, help="The file containing the input texts to the model")
    arg_parser.add_argument('--summaries', '-s', type=str, required=True, help="The file containing the summaries generated by the model")
    arg_parser.add_argument('--references', '-r', type=str, required=True, help="The file containing the references for the summaries")
    arg_parser.add_argument('--metric', '-m', type=str, required=True, choices=AVAILABLE_METRICS, help="The metric to evaluate")
    args = arg_parser.parse_args()

    # Load the input texts, summaries, and references
    input_texts = load_text_file(args.input_texts)
    summaries = load_text_file(args.summaries)
    references = load_text_file(args.references)
    metric = args.metric
    device = 'cpu'  # TODO: use a GPU

    # Evaluate the metrics
    results = run_evaluation(input_texts, summaries, references, device, metric)
    print(json.dumps(results, indent=2))
